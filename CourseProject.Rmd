---
title: "Course Project - Modeling Data in the Tidyverse"
author: "dillonchewwx"
date: "15/03/2021"
output:
    prettydoc::html_pretty:
        theme: tactile
        highlight: github
---

# Overview

In this project, the consumer complains data from the [Consumer Complaint Database](https://www.consumerfinance.gov/data-research/consumer-complaints/) (CFPB) would be used. The CFPB is an independent agency of the United States government that promotes transparency and protects consumers by providing information needed to make decisions when choosing financial institutions including banking institutions, lenders, mortgage services, credit unions, securities firms, foreclosure services, and debt collectors. One of the purposes of the agency is to receive and process complaints and questions about consumer financial products and services. 

When a complaint is submitted by a consumer, the CFPB has to determine which category the complaint falls in (e.g. "Mortgage", "Student loan", etc). In this project, the goal would be to build a classification algorithm to classify consumer complaints into one of four categories: "Credit card or prepaid card", "Mortgage", "Student loan", or "Vehicle loan or lease".

The datasets can be downloaded from the following links:

* [Training Data](https://d3c33hcgiwev3.cloudfront.net/JhHJz2SSRCqRyc9kkgQqxA_8d34147955154de4a6176086946d07b3_data_complaints_train.csv?Expires=1615939200&Signature=gOhyZos3R0Iyd~INVdwxNohGqi1uQsqdwJpjJdX11qeGOk~uM~YxC9YmhDjnkad1ykcujc2QtpJr-90NUKNix~tqX4~4c-FhuPRnWkktqumLNSMxDRaoqAD7cNQWm01IbOUPWuVKrzFI-EJGiUTqIeNO-2C5IFx4R7eYb011oJU_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A)
* [Testing Data](https://d3c33hcgiwev3.cloudfront.net/aEBWUxehSGyAVlMXoThsoQ_edf53641edca416fa00a78d9e4b16ced_data_complaints_test.csv?Expires=1615939200&Signature=AVVoqIcMXOMYB2~VngrnderA2i4TKMhIC7BIrWRoXQWhmWSAjwFrlvxwczLhXKqBLsWDoh52elxjr-cNydTKY7w1BeB7dOjKGcPuVxqkmVJ-lrqaKeSyWM0ctjeHFw3WYomXYhqSVBz91QQp4-IsmnH08Z1rKB4xMVvWXyi9tK0_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A) 

# Data Import
```{r Data Import}
library(tidyverse) # for data frame manipulation
library(tidymodels) # for ML 
library(tm) # for text manipulation

df<-read_csv("../data_complaints_train.csv")
```
## Data Inspection
```{r Data Inspection}
glimpse(df)
```
A quick look of the data reveals the following:

* The outcome variable which we are interested in is called `Product`.
* Although there are 5 variables which can be used as predictors, the `Consumer complaint narrative` variable seems to provide the most relevant information for making predictions on the product category. 

Thus, we shall only select the above two variables for further processing. 
```{r Extract Relevant Data}
# Rename the complaint column and only select the two variables
df<-df %>%
    rename(Complain=`Consumer complaint narrative`) %>%
    select(Product, Complain)
```
# Data Pre-processing
Let's take a closer look at each of the variables.
```{r Check Variables}
# Check if there are any categories
unique(df$Product)
# Check the first few rows of Complain
head(df$Complain)
```

It appears that `Product` only has 4 categories. We can make it into a factor. However for `Complain`, it is a long narrative which is much more complicated and thus we have to clean it up. We will carry out the following clean-up process:

1. Drop any rows with `NA` in the column.
2. Remove any strings such as "XX", "XXX" and "XXXX" which were used to mask private information and dates.
3. Convert all letters to lower case.
4. Remove any numbers.
5. Remove all punctuation.
6. Remove escape characters and white spaces: `\t` and `\n`.
7. Remove stop words e.g. "I", "My", "which", "won't" etc..
```{r Complain Cleanup}
df_clean<-df %>%
    filter(Complain!=is.na(.)) %>%
    mutate(Complain=gsub("[XX]+", "", Complain)) %>%
    mutate(Complain=str_to_lower(Complain)) %>%
    mutate(Complain=gsub("[0-9]", "", Complain)) %>%
    mutate(Complain=removePunctuation(Complain)) %>%
    mutate(Complain=gsub("\n", "", Complain)) %>%
    mutate(Complain=gsub("\t", "", Complain)) %>%
    mutate(Complain=stripWhitespace(Complain))
complains<-Corpus(VectorSource(df_clean$Complain)) %>%
    tm_map(removeWords, stopwords())
```
We will now create a document term matrix using the `tm` package where each complaint will be a "document" occupying a row, each "term" will be a column name, and the counts of each term per document will be the values.
```{r Create DTM}
dtm<-DocumentTermMatrix(complains)
inspect(dtm)
```
We note that there are 90975 documents and 81653 terms - that's huge! We shall try to reduce our data set initially by keeping only the relevant terms for exploratory analysis. Here, we define relevant terms as terms which appear 1000 times or more.
```{r Keep Frequent Terms}
# Find terms which appear 1000 times or more
freqTerms<-findFreqTerms(dtm, lowfreq=1000)
# Limit DTM to only contain terms that appear 1000 times or more.
dtm<-DocumentTermMatrix(complains, list(dictionary=freqTerms))
inspect(dtm)
```
We have reduced the number of terms to 1552, but the sparsity has dropped marginally to 96%. Let's further simplify by removing sparse terms which have at least 95% of sparse elements (i.e, terms only occuring 0 times in a document) which can help to reduce the matrix without losing significant relations inherent to the matrix. 
```{r Remove Sparse Terms}
dtm<-removeSparseTerms(dtm, 0.95)
inspect(dtm)
```
We have reduced the number of terms to 341, and the sparsity is now 89%. Following which, lets now carry out some simple exploratory analysis on the dataset. We can start by finding out the top 150 terms. 
```{r Exploratory Analysis - Top 150 terms}
# Convert to Matrix for further processing
dtm_mat<-as.matrix(dtm)

# Get the total number of counts for each term and show the top 150 terms
sumTerm<-colSums(dtm_mat) %>%
    .[order(-.)]
head(sumTerm, 150)
```
It appears that there are several other frequent words in the top 150 which may not add value to the data set e.g. "told", "called", "back", "get", "will" etc. Let's remove them to make the data cleaner. 
```{r Remove extra stop words}
extraStopWords<-c("told", "called", "back", "get", "will", "never", "said", "can", "call", "now", "also",
                  "even", "just", "like", "please", "take", "want", "going", "without", "got", "however", 
                  "went", "able", "didnt", "dont", "put", "later", "way", "done", "needed", "today", "used", "took")
complains<-tm_map(complains, removeWords, extraStopWords)
```
Now, we shall recreate the DTM, but with all the additional filters. 
```{r Create New DTM}
# Create a new DTM with all previous filters.
dtm<-DocumentTermMatrix(complains, list(dictionary=freqTerms))
inspect(dtm)
dtm<-removeSparseTerms(dtm, 0.95)
inspect(dtm)
```
We have reduced the number of terms to 308.

# Building a ML model for predicting product category

## Create Train and Test Sets
As we have briefly processed the "train" data set as dtm previously, we would now need to split it into training (75%) and test (25%) sets. Let's start by creating the training data set. 
```{r Create Train Data Set}
# Convert Product to a factor
df_clean<-df_clean %>%
    mutate(Product=factor(Product))

# Convert dtm_mat into a data frame and add the outcome Product column.
dtm_train<-dtm_mat %>%
    as.matrix() %>%
    as.data.frame() %>% 
    bind_cols(Product=df_clean$Product) %>% 
    select(Product, everything())

# Create the training data set
set.seed(1234)
split_dtm<-initial_split(dtm_train, strata=Product, prop=3/4)
training_dtm<-training(split_dtm)
head(training_dtm)
count(training_dtm, Product)
```
We shall do the same for the testing data set.
```{r Create Test Data Set}
testing_dtm<-testing(split_dtm)
head(testing_dtm)
count(testing_dtm, Product)
```
## Create cross validation folds.
Before moving on, we can also create cross validation folds with `rsample`. We will carry out a 10-fold cross-validation.
```{r Cross-Validation}
vfold_dtm<-vfold_cv(data=training_dtm, v=10)
vfold_dtm
pull(vfold_dtm, splits)

# Check first fold
first_fold<-vfold_dtm$splits[[1]]
# Training set of this fold
head(as.data.frame(first_fold, data="analysis")) 
# Test set of this fold
head(as.data.frame(first_fold, data="assessment")) 
```
## Creating ML workflow
We can now create a recipe, model, and workflow for the ML model. We will use a classification decision tree model (`rpart` engine) as our variable is categorical. 
```{r Recipe, Model , Workflow}
# Create a recipe
dtm_recipe<-training_dtm %>%
    recipe(Product~.)

# Create a model
dtm_model<-decision_tree() %>%
    set_mode("classification") %>%
    set_engine("rpart")
dtm_model

# Create a workflow
dtm_workflow<-workflow() %>%
    add_recipe(dtm_recipe) %>%
    add_model(dtm_model) 
dtm_workflow
```
## Fit and assess model performance
Now that the setup is done, we shall attempt to fit the workflow.
```{r Fitting}
# Fit without cross validation
dtm_fit<-fit(dtm_workflow, data=training_dtm)
dtm_workflow_fit<-dtm_fit %>%
    pull_workflow_fit()
dtm_workflow_fit

dtm_workflow_fit$fit$variable.importance
```
Here, we can see that `mortgage` was the most important word for predicting Product. Following this, we can perform some predictions.
```{r Predictions}
predict_Product<-predict(dtm_workflow_fit, new_data=training_dtm)
accuracy(training_dtm, truth=Product, estimate=predict_Product$.pred_class)

# Breakdown of Product 
count(training_dtm, Product)
# Breakdown of Predicted Product
count(predict_Product, .pred_class)
# Show which rows were predicted correctly
predicted_and_truth<-bind_cols(training_dtm, predicted_Product=pull(predict_Product, .pred_class)) %>%
    select(predicted_Product, everything())
head(predicted_and_truth)
```
The accuracy is about 84.1%. Seems pretty alright on the first go. We also note that extra "Credit card or prepaid card" and "Mortgage" was predicted, while "Student loan" and "Vehicle loan or lease" was under predicted. Additionally, we can also see exactly which rows resulted in incorrect predictions. 

Lets now try to fit the model to our cross validation folds.
```{r vfold Fit}
set.seed(122)
resample_fit<-fit_resamples(dtm_workflow, vfold_dtm)
collect_metrics(resample_fit)
```
Interestingly, the model accuracy stays the same. 

## Tuning the model
Now, lets try to improve the model by tuning a hyperparameter. 
```{r Tuning}
# Create tuned model
dtm_model_tune<-decision_tree(cost_complexity=tune(), tree_depth=tune()) %>%
    set_mode("classification") %>%
    set_engine("rpart")

tree_grid<-grid_regular(cost_complexity(), tree_depth(), levels=3)

# Create workflow
dtm_workflow_tune<-workflow() %>%
    add_recipe(dtm_recipe) %>%
    add_model(dtm_model_tune)

# Perform tuning
set.seed(123)
model_resample_fit<-tune_grid(dtm_workflow_tune, 
                              resamples=vfold_dtm, 
                              grid=tree_grid, 
                              metrics=metric_set(accuracy, roc_auc))

# Assess Performance
show_best(model_resample_fit, metric="accuracy")
```
Seems like tuning doesn't make any difference to the model - the highest accuracy achieved is 84.1%. We can stop the model building here and just update `dtm_workflow_tune` with the values chosen by `select_best()`.
```{r Update final model}
# Specify min_n value
tuned_values<-select_best(model_resample_fit,"accuracy")
# Finalize model/workflow
dtm_workflow_tuned<-dtm_workflow_tune %>%
    finalize_workflow(tuned_values)
```

# Final Model Performance Evaluation
We shall first fit the final model on the full training set and also on the testing data using the `last_fit()` function. 
```{r Evaluation}
# Fit final model on the full training set
overall_fit<-last_fit(dtm_workflow_tuned, split_dtm)
collect_metrics(overall_fit)
```
The accuracy has increased slightly to 84.4%.

Let's take a look at the predicted values for the test set. 
```{r Predict Test Set}
test_predictions<-collect_predictions(overall_fit)
head(test_predictions)
```

We can do a plot to better visualize the predictions.
```{r Prediction Plot}
ggplot(test_predictions, aes(x=Product, fill=.pred_class)) +
    geom_bar(position="fill", color="black") + 
    scale_fill_brewer(palette="Set3") + 
    labs(x="Actual Outcome Values", 
         y="Proportion", 
         fill="Predicted Outcome") + 
    theme_bw() + 
    theme(axis.text.x=element_text(angle=45, hjust=1, vjust=1))
```

It appears that our model does relatively well on all outcomes except for "Vehicle loan or lease". 

# Predicting outcomes for the the provided test data
Let's start by loading the data and doing some pre-processing.
```{r Prediction - Preprocessing}
# Load and clean up the data
df_test<-read_csv("../data_complaints_test.csv") %>%
    rename(Complain=`Consumer complaint narrative`) %>%
    select(Complain) %>%
    mutate(Complain=gsub("[XX]+", "", Complain)) %>%
    mutate(Complain=str_to_lower(Complain)) %>%
    mutate(Complain=gsub("[0-9]", "", Complain)) %>%
    mutate(Complain=removePunctuation(Complain)) %>%
    mutate(Complain=gsub("\n", "", Complain)) %>%
    mutate(Complain=gsub("\t", "", Complain)) %>%
    mutate(Complain=stripWhitespace(Complain))

# Make corpus
test_complains<-Corpus(VectorSource(df_test$Complain)) %>%
    tm_map(removeWords, stopwords())

# Make DTM
dtm_test<-DocumentTermMatrix(test_complains, list(dictionary=freqTerms))
dtm_test_mat<-dtm_test %>%
    as.matrix() %>%
    as.data.frame()
head(dtm_test_mat)
```
Now its time to make predictions!
```{r Prediction Time}
final_model<-fit(dtm_workflow_tuned, dtm_train)
predict(final_model, new_data=dtm_test_mat)
```